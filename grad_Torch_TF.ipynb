{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative with respect to x: \n",
      "tensor([[ 1.,  0., -1.],\n",
      "        [ 2.,  0., -2.],\n",
      "        [ 1.,  0., -1.]])\n",
      "Derivative with respect to f: \n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define input data and filters\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], requires_grad=True)\n",
    "f = torch.tensor([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], requires_grad=True)\n",
    "\n",
    "# Define the forward pass function\n",
    "def conv2d(x, f):\n",
    "    return torch.nn.functional.conv2d(x.view(1, 1, 3, 3), f.view(1, 1, 3, 3), padding=0)\n",
    "\n",
    "# Define the loss function (sum of the output values)\n",
    "def loss(y):\n",
    "    return y.sum()\n",
    "\n",
    "# Compute the derivative of the loss with respect to x and f using autograd\n",
    "y = conv2d(x, f)\n",
    "L = loss(y)\n",
    "grads = torch.autograd.grad(L, [x, f])\n",
    "\n",
    "# Print the derivative values\n",
    "print(\"Derivative with respect to x: \")\n",
    "print(grads[0])\n",
    "print(\"Derivative with respect to f: \")\n",
    "print(grads[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative with respect to x: \n",
      "tensor([[[[ 1.,  0., -1.],\n",
      "          [ 2.,  0., -2.],\n",
      "          [ 1.,  0., -1.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Derivative with respect to f: \n",
      "tensor([[[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define input data and filters\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], requires_grad=True)\n",
    "f = torch.tensor([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], requires_grad=True)\n",
    "\n",
    "# Define the forward pass function\n",
    "def conv2d(x, f):\n",
    "    return torch.nn.functional.conv2d(x.view(1, 1, 3, 3), f.view(1, 1, 3, 3), padding=0)\n",
    "\n",
    "# Define the loss function (sum of the output values)\n",
    "def loss(y):\n",
    "    return y.sum()\n",
    "\n",
    "# Compute the derivative of the loss with respect to x and f using PyTorch's low-level functions\n",
    "y = conv2d(x, f)\n",
    "L = loss(y)\n",
    "\n",
    "# Compute the derivative of L with respect to y\n",
    "grad_y = torch.ones_like(y)\n",
    "#grad_y *= L\n",
    "\n",
    "# Compute the derivative of L with respect to x and f using conv2d_transpose\n",
    "grad_x = torch.nn.functional.conv_transpose2d(grad_y, f.view(1, 1, 3, 3), padding=0)\n",
    "grad_f = torch.nn.functional.conv2d(x.view(1, 1, 3, 3), grad_y, padding=0)\n",
    "\n",
    "# Print the derivative values\n",
    "print(\"Derivative with respect to x: \")\n",
    "print(grad_x)\n",
    "print(\"Derivative with respect to f: \")\n",
    "print(grad_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[[-8.]]]], shape=(1, 1, 1, 1), dtype=float32)\n",
      "Derivative with respect to x: \n",
      "tf.Tensor(\n",
      "[[ 1.  0. -1.]\n",
      " [ 2.  0. -2.]\n",
      " [ 1.  0. -1.]], shape=(3, 3), dtype=float32)\n",
      "Derivative with respect to f: \n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define input data and filters\n",
    "x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], dtype=tf.float32)\n",
    "f = tf.constant([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], dtype=tf.float32)\n",
    "\n",
    "# Define the forward pass function\n",
    "def conv2d(x, f):\n",
    "    return tf.nn.conv2d(tf.reshape(x, [1, 3, 3, 1]), tf.reshape(f, [3, 3, 1, 1]), strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "# Define the loss function (sum of the output values)\n",
    "def loss(y):\n",
    "    return tf.reduce_sum(y)\n",
    "\n",
    "# Compute the derivative of the loss with respect to x and f using tape gradient\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x, f])\n",
    "    y = conv2d(x, f)\n",
    "    #L = loss(y)\n",
    "    print(y)\n",
    "grads = tape.gradient(y, [x, f])\n",
    "\n",
    "# Print the derivative values\n",
    "print(\"Derivative with respect to x: \")\n",
    "print(grads[0])\n",
    "print(\"Derivative with respect to f: \")\n",
    "print(grads[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of convolution: \n",
      "tf.Tensor([[[[-8.]]]], shape=(1, 1, 1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define input data and filters\n",
    "x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], dtype=tf.float32)\n",
    "f = tf.constant([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], dtype=tf.float32)\n",
    "\n",
    "# Perform the 2D convolution\n",
    "y = tf.nn.conv2d(tf.reshape(x, [1, 3, 3, 1]), tf.reshape(f, [3, 3, 1, 1]), strides=[1, 1, 1, 1], padding='VALID')\n",
    "print(\"Output of convolution: \")\n",
    "print(y)\n",
    "\n",
    "# Define the derivative of the convolution operation\n",
    "x_grad = tf.nn.conv2d_transpose(y, tf.reshape(f, [3, 3, 1, 1]), tf.shape(x), strides=[1, 1, 1, 1], padding='VALID')\n",
    "f_grad = tf.nn.conv2d(tf.reshape(x, [1, 3, 3, 1]), tf.transpose(y, perm=[1, 2, 0, 3]), strides=[1, 1, 1, 1], padding='VALID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\n",
       "array([[[[ -8.],\n",
       "         [  0.],\n",
       "         [  8.]],\n",
       "\n",
       "        [[-16.],\n",
       "         [  0.],\n",
       "         [ 16.]],\n",
       "\n",
       "        [[ -8.],\n",
       "         [  0.],\n",
       "         [  8.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def convolve2d_gradient(x, w, d):\n",
    "    \"\"\"\n",
    "    Computes the gradient of 2D convolution with respect to x (dx) and the filter (dw).\n",
    "\n",
    "    Args:\n",
    "    x: 2D input array of shape (H, W)\n",
    "    w: 2D filter array of shape (FH, FW)\n",
    "    d: 2D output gradient array of shape (OH, OW)\n",
    "\n",
    "    Returns:\n",
    "    dx: 2D gradient of x array of shape (H, W)\n",
    "    dw: 2D gradient of w array of shape (FH, FW)\n",
    "    \"\"\"\n",
    "\n",
    "    # Flip the filter in both directions for cross-correlation\n",
    "    w_flipped = np.flip(np.flip(w, axis=0), axis=1)\n",
    "\n",
    "    # Compute the gradient of x using the transposed filter\n",
    "    dx = convolve2d(d, w_flipped, mode='full')[:x.shape[0], :x.shape[1]]\n",
    "\n",
    "    # Compute the gradient of w using the input and output gradients\n",
    "    dw = convolve2d(np.flip(np.flip(x, axis=0), axis=1), d, mode='valid')\n",
    "\n",
    "    return dx, dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x:\n",
      "[[1 2 0]\n",
      " [3 5 2]\n",
      " [0 3 4]]\n",
      "Gradient of w:\n",
      "[[77 67]\n",
      " [47 37]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "w = np.array([[1, 0], [0, 1]])\n",
    "d = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "dx, dw = convolve2d_gradient(x, w, d)\n",
    "\n",
    "print(\"Gradient of x:\")\n",
    "print(dx)\n",
    "\n",
    "print(\"Gradient of w:\")\n",
    "print(dw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rot180(w):\n",
    "    \"\"\"\n",
    "    Roate by 180 degrees\n",
    "    \"\"\"\n",
    "    return torch.flip(w, dims=[2, 3])\n",
    "\n",
    "def pad_to_full_conv2d(x, w_size):\n",
    "    \"\"\"\n",
    "    Pad x, such that using a 'VALID' convolution in PyTorch is the same\n",
    "    as using a 'FULL' convolution.\n",
    "    \"\"\"\n",
    "    padding = w_size - 1\n",
    "    return F.pad(x, (padding, padding, padding, padding), mode='constant', value=0)\n",
    "\n",
    "def NHWC_to_HWIO(out):\n",
    "    \"\"\"\n",
    "    Converts [batch, in_channels, in_height, in_width]\n",
    "    to       [out_channels, in_channels, filter_height, filter_width]\n",
    "    \"\"\"\n",
    "    return out.permute(3, 2, 1, 0)\n",
    "\n",
    "# sizes, fixed strides, in_channel, out_channel be 1 for now\n",
    "x_size = 4\n",
    "w_size = 3  # use an odd number here\n",
    "x_shape = (1, 1, x_size, x_size)\n",
    "w_shape = (1, 1, w_size, w_size)\n",
    "out_shape = (1, 1, x_size - w_size + 1, x_size - w_size + 1)\n",
    "strides = (1, 1)\n",
    "\n",
    "# numpy value\n",
    "x_np = torch.randint(10, size=x_shape, dtype=torch.float32)\n",
    "w_np = torch.randint(10, size=w_shape, dtype=torch.float32)\n",
    "out_scale_np = torch.randint(10, size=out_shape, dtype=torch.float32)\n",
    "\n",
    "# pytorch forward\n",
    "x = torch.tensor(x_np, requires_grad=True)\n",
    "w = torch.tensor(w_np, requires_grad=True)\n",
    "out = F.conv2d(x, w, stride=strides, padding=0)\n",
    "out_scale = torch.tensor(out_scale_np, requires_grad=True)\n",
    "f = torch.sum(out * out_scale)\n",
    "\n",
    "# pytorch backward\n",
    "f.backward(retain_graph=True)\n",
    "d_out = x.grad.detach()\n",
    "\n",
    "# 4 different ways to compute d_x\n",
    "d_x = x.grad\n",
    "d_x_manual = F.conv2d(d_out, w.flip([2, 3]), stride=strides, padding=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1509., 1680.],\n",
       "          [ 930., 1288.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_x_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[25., 30., 25.,  0.],\n",
       "          [30., 47., 79., 20.],\n",
       "          [13., 27., 65., 36.],\n",
       "          [ 2.,  6., 18., 28.]]]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define input tensor and filters\n",
    "x = Variable(torch.randn(3, 1, 3, 3), requires_grad=True)\n",
    "w = Variable(torch.randn(3, 1, 3, 3), requires_grad=True)\n",
    "\n",
    "# Perform convolution operation\n",
    "y = F.conv2d(x, w, stride=1, padding=1)\n",
    "\n",
    "# Define loss function\n",
    "loss = y.sum()\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients of input and weights\n",
    "grad_x = x.grad\n",
    "grad_w = w.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -8.9880, -10.5122,  -4.5570],\n",
       "          [ -5.1470,  -7.5202,  -4.7913],\n",
       "          [  2.8497,   0.1662,  -2.4810]]],\n",
       "\n",
       "\n",
       "        [[[ -8.9880, -10.5122,  -4.5570],\n",
       "          [ -5.1470,  -7.5202,  -4.7913],\n",
       "          [  2.8497,   0.1662,  -2.4810]]],\n",
       "\n",
       "\n",
       "        [[[ -8.9880, -10.5122,  -4.5570],\n",
       "          [ -5.1470,  -7.5202,  -4.7913],\n",
       "          [  2.8497,   0.1662,  -2.4810]]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -5.2912,  -9.7557,  -5.3568],\n",
       "          [ -6.1425, -11.4750,  -5.6802],\n",
       "          [ -3.0966,  -5.6679,  -2.7107]]],\n",
       "\n",
       "\n",
       "        [[[ -5.2912,  -9.7557,  -5.3568],\n",
       "          [ -6.1425, -11.4750,  -5.6802],\n",
       "          [ -3.0966,  -5.6679,  -2.7107]]],\n",
       "\n",
       "\n",
       "        [[[ -5.2912,  -9.7557,  -5.3568],\n",
       "          [ -6.1425, -11.4750,  -5.6802],\n",
       "          [ -3.0966,  -5.6679,  -2.7107]]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define input tensor and filters\n",
    "x = torch.randn(3, 3, 5, 5, requires_grad=True)\n",
    "w = torch.randn(3, 3, 3, 3, requires_grad=True)\n",
    "\n",
    "# Perform convolution operation\n",
    "y = F.conv2d(x, w, stride=1, padding=1)\n",
    "\n",
    "# Define loss function\n",
    "loss = y.sum()\n",
    "\n",
    "# Compute gradients of output tensor\n",
    "grad_y = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
    "\n",
    "# Compute gradients of input tensor and filters\n",
    "grad_x = F.conv_transpose2d(grad_y,w, stride=1, padding=1)\n",
    "grad_w = F.conv2d(x,grad_y,stride=1,padding=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 5, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.0718, -3.6533, -3.6533, -3.6533, -1.4389],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-1.9459, -3.7799, -3.7799, -3.7799, -1.8813]],\n",
       "\n",
       "         [[-1.5236,  0.8639,  0.8639,  0.8639,  3.1336],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [ 0.9425,  3.4974,  3.4974,  3.4974,  0.3375]],\n",
       "\n",
       "         [[-5.8785, -5.5987, -5.5987, -5.5987, -1.8357],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-4.2066, -4.6123, -4.6123, -4.6123, -5.2918]]],\n",
       "\n",
       "\n",
       "        [[[-3.0718, -3.6533, -3.6533, -3.6533, -1.4389],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-1.9459, -3.7799, -3.7799, -3.7799, -1.8813]],\n",
       "\n",
       "         [[-1.5236,  0.8639,  0.8639,  0.8639,  3.1336],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [ 0.9425,  3.4974,  3.4974,  3.4974,  0.3375]],\n",
       "\n",
       "         [[-5.8785, -5.5987, -5.5987, -5.5987, -1.8357],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-4.2066, -4.6123, -4.6123, -4.6123, -5.2918]]],\n",
       "\n",
       "\n",
       "        [[[-3.0718, -3.6533, -3.6533, -3.6533, -1.4389],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-1.9459, -3.7799, -3.7799, -3.7799, -1.8813]],\n",
       "\n",
       "         [[-1.5236,  0.8639,  0.8639,  0.8639,  3.1336],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [ 0.9425,  3.4974,  3.4974,  3.4974,  0.3375]],\n",
       "\n",
       "         [[-5.8785, -5.5987, -5.5987, -5.5987, -1.8357],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-4.2066, -4.6123, -4.6123, -4.6123, -5.2918]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -9.8554, -10.0831,  -8.2008],\n",
       "          [ -9.1956,  -9.3712,  -5.0751],\n",
       "          [ -8.3023,  -8.3388,  -3.4559]],\n",
       "\n",
       "         [[ -9.8554, -10.0831,  -8.2008],\n",
       "          [ -9.1956,  -9.3712,  -5.0751],\n",
       "          [ -8.3023,  -8.3388,  -3.4559]],\n",
       "\n",
       "         [[ -9.8554, -10.0831,  -8.2008],\n",
       "          [ -9.1956,  -9.3712,  -5.0751],\n",
       "          [ -8.3023,  -8.3388,  -3.4559]]],\n",
       "\n",
       "\n",
       "        [[[ 16.6867,  17.7942,  10.9246],\n",
       "          [  9.7287,  10.0513,   4.0828],\n",
       "          [  7.8516,   6.7181,   3.6140]],\n",
       "\n",
       "         [[ 16.6867,  17.7942,  10.9246],\n",
       "          [  9.7287,  10.0513,   4.0828],\n",
       "          [  7.8516,   6.7181,   3.6140]],\n",
       "\n",
       "         [[ 16.6867,  17.7942,  10.9246],\n",
       "          [  9.7287,  10.0513,   4.0828],\n",
       "          [  7.8516,   6.7181,   3.6140]]],\n",
       "\n",
       "\n",
       "        [[[  5.7695,   1.1255,   1.1586],\n",
       "          [  5.2874,   0.5180,   2.1058],\n",
       "          [  8.9798,   6.4206,   7.8446]],\n",
       "\n",
       "         [[  5.7695,   1.1255,   1.1586],\n",
       "          [  5.2874,   0.5180,   2.1058],\n",
       "          [  8.9798,   6.4206,   7.8446]],\n",
       "\n",
       "         [[  5.7695,   1.1255,   1.1586],\n",
       "          [  5.2874,   0.5180,   2.1058],\n",
       "          [  8.9798,   6.4206,   7.8446]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients of input and weights\n",
    "grad_x = x.grad\n",
    "grad_w = w.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.0718, -3.6533, -3.6533, -3.6533, -1.4389],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-1.9459, -3.7799, -3.7799, -3.7799, -1.8813]],\n",
       "\n",
       "         [[-1.5236,  0.8639,  0.8639,  0.8639,  3.1336],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [ 0.9425,  3.4974,  3.4974,  3.4974,  0.3375]],\n",
       "\n",
       "         [[-5.8785, -5.5987, -5.5987, -5.5987, -1.8357],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-4.2066, -4.6123, -4.6123, -4.6123, -5.2918]]],\n",
       "\n",
       "\n",
       "        [[[-3.0718, -3.6533, -3.6533, -3.6533, -1.4389],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-1.9459, -3.7799, -3.7799, -3.7799, -1.8813]],\n",
       "\n",
       "         [[-1.5236,  0.8639,  0.8639,  0.8639,  3.1336],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [ 0.9425,  3.4974,  3.4974,  3.4974,  0.3375]],\n",
       "\n",
       "         [[-5.8785, -5.5987, -5.5987, -5.5987, -1.8357],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-4.2066, -4.6123, -4.6123, -4.6123, -5.2918]]],\n",
       "\n",
       "\n",
       "        [[[-3.0718, -3.6533, -3.6533, -3.6533, -1.4389],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-3.5878, -5.2947, -5.2947, -5.2947, -1.9294],\n",
       "          [-1.9459, -3.7799, -3.7799, -3.7799, -1.8813]],\n",
       "\n",
       "         [[-1.5236,  0.8639,  0.8639,  0.8639,  3.1336],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [-1.0251,  2.7720,  2.7720,  2.7720,  3.0399],\n",
       "          [ 0.9425,  3.4974,  3.4974,  3.4974,  0.3375]],\n",
       "\n",
       "         [[-5.8785, -5.5987, -5.5987, -5.5987, -1.8357],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-6.8675, -8.8960, -8.8960, -8.8960, -7.4468],\n",
       "          [-4.2066, -4.6123, -4.6123, -4.6123, -5.2918]]]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.7395,  2.8424,  0.5553],\n",
       "          [-2.2336, -3.6704, -3.1803],\n",
       "          [-3.5062, -4.7763, -2.5760]],\n",
       "\n",
       "         [[-4.0514, -6.9727, -6.8359],\n",
       "          [-1.2134, -3.3513, -1.9261],\n",
       "          [ 5.6523,  5.6591,  8.4727]],\n",
       "\n",
       "         [[11.9128, 12.9668, 10.1629],\n",
       "          [ 9.2675,  8.2198,  6.2199],\n",
       "          [ 6.3831,  3.9171,  2.1060]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7395,  2.8424,  0.5553],\n",
       "          [-2.2336, -3.6704, -3.1803],\n",
       "          [-3.5062, -4.7763, -2.5760]],\n",
       "\n",
       "         [[-4.0514, -6.9727, -6.8359],\n",
       "          [-1.2134, -3.3513, -1.9261],\n",
       "          [ 5.6523,  5.6591,  8.4727]],\n",
       "\n",
       "         [[11.9128, 12.9668, 10.1629],\n",
       "          [ 9.2675,  8.2198,  6.2199],\n",
       "          [ 6.3831,  3.9171,  2.1060]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7395,  2.8424,  0.5553],\n",
       "          [-2.2336, -3.6704, -3.1803],\n",
       "          [-3.5062, -4.7763, -2.5760]],\n",
       "\n",
       "         [[-4.0514, -6.9727, -6.8359],\n",
       "          [-1.2134, -3.3513, -1.9261],\n",
       "          [ 5.6523,  5.6591,  8.4727]],\n",
       "\n",
       "         [[11.9128, 12.9668, 10.1629],\n",
       "          [ 9.2675,  8.2198,  6.2199],\n",
       "          [ 6.3831,  3.9171,  2.1060]]]])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_x = F.conv2d(grad_y,w.flip([2,3]),padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.0973,  6.3338,  6.3338,  6.3338,  5.3199],\n",
       "          [ 7.9552, 11.1193, 11.1193, 11.1193,  8.0118],\n",
       "          [ 7.9552, 11.1193, 11.1193, 11.1193,  8.0118],\n",
       "          [ 7.9552, 11.1193, 11.1193, 11.1193,  8.0118],\n",
       "          [ 6.5303,  7.7477,  7.7477,  7.7477,  4.1833]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x:\n",
      "tensor([[[[-3.9087, -2.5821, -2.5821, -2.5821, -0.2595],\n",
      "          [-3.3397, -3.2095, -3.2095, -3.2095, -0.7135],\n",
      "          [-3.3397, -3.2095, -3.2095, -3.2095, -0.7135],\n",
      "          [-3.3397, -3.2095, -3.2095, -3.2095, -0.7135],\n",
      "          [-0.5627, -2.0320, -2.0320, -2.0320, -0.0084]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[[ -5.2275,  -2.9276,  -2.5082],\n",
      "          [ -9.0215,  -4.8723,  -3.1846],\n",
      "          [-10.2993,  -6.0862,  -4.9238]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create some input data and a convolution kernel\n",
    "x = torch.randn(1, 1, 5, 5, requires_grad=True)\n",
    "w = torch.randn(1, 1, 3, 3, requires_grad=True)\n",
    "\n",
    "# Perform the convolution\n",
    "y = F.conv2d(x, w,stride=1,padding=1)\n",
    "loss = y.sum()\n",
    "# Compute the gradient of the convolution output with respect to the input\n",
    "#grad_y = torch.ones_like(y)\n",
    "grad_y = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
    "\n",
    "grad_x = F.conv_transpose2d(grad_y, w, stride=1, padding=1)\n",
    "grad_w = F.conv2d(x,grad_y,stride=1,padding=1)\n",
    "# Print the gradient of y with respect to x\n",
    "print(\"Gradient of y with respect to x:\")\n",
    "print(grad_x)\n",
    "print(grad_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-3.9087, -2.5821, -2.5821, -2.5821, -0.2595],\n",
      "          [-3.3397, -3.2095, -3.2095, -3.2095, -0.7135],\n",
      "          [-3.3397, -3.2095, -3.2095, -3.2095, -0.7135],\n",
      "          [-3.3397, -3.2095, -3.2095, -3.2095, -0.7135],\n",
      "          [-0.5627, -2.0320, -2.0320, -2.0320, -0.0084]]]])\n",
      "tensor([[[[ -5.2275,  -2.9276,  -2.5082],\n",
      "          [ -9.0215,  -4.8723,  -3.1846],\n",
      "          [-10.2993,  -6.0862,  -4.9238]]]])\n"
     ]
    }
   ],
   "source": [
    "loss = y.sum()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients of input and weights\n",
    "grad_x = x.grad\n",
    "grad_w = w.grad\n",
    "print(grad_x)\n",
    "print(grad_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.9456, -1.0765, -1.0765, -1.0765, -0.9311],\n",
       "          [ 1.7023, -3.1691, -3.1691, -3.1691, -3.4197],\n",
       "          [ 1.7023, -3.1691, -3.1691, -3.1691, -3.4197],\n",
       "          [ 1.7023, -3.1691, -3.1691, -3.1691, -3.4197],\n",
       "          [ 1.1934, -1.7279, -1.7279, -1.7279, -2.1491]]]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x = F.conv2d(grad_y, w.flip([2,3]), stride=1, padding=1)\n",
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x using auto-differentiation:\n",
      "tensor([[[[-2.1584, -2.1899, -2.1899, -2.1899, -1.1411],\n",
      "          [-4.0398, -2.6966, -2.6966, -2.6966,  0.4368],\n",
      "          [-4.0398, -2.6966, -2.6966, -2.6966,  0.4368],\n",
      "          [-4.0398, -2.6966, -2.6966, -2.6966,  0.4368],\n",
      "          [-4.0967, -1.7770, -1.7770, -1.7770,  0.6559]]]])\n",
      "Gradient of y with respect to x using manual computation:\n",
      "tensor([[[[-2.1584, -2.1899, -2.1899, -2.1899, -1.1411],\n",
      "          [-4.0398, -2.6966, -2.6966, -2.6966,  0.4368],\n",
      "          [-4.0398, -2.6966, -2.6966, -2.6966,  0.4368],\n",
      "          [-4.0398, -2.6966, -2.6966, -2.6966,  0.4368],\n",
      "          [-4.0967, -1.7770, -1.7770, -1.7770,  0.6559]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "Gradient of y with respect to w using auto-differentiation:\n",
      "tensor([[[[ 4.6517,  4.9953,  1.5210],\n",
      "          [ 5.8352,  4.6035,  0.5456],\n",
      "          [ 4.3783,  4.0135, -0.5564]]]])\n",
      "Gradient of y with respect to w using manual computation:\n",
      "tensor([[[[ 4.6517,  4.9953,  1.5210],\n",
      "          [ 5.8352,  4.6035,  0.5456],\n",
      "          [ 4.3783,  4.0135, -0.5564]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create some input data and a convolution kernel\n",
    "x = torch.randn(1, 1, 5, 5, requires_grad=True)\n",
    "w = torch.randn(1, 1, 3, 3, requires_grad=True)\n",
    "\n",
    "# Perform the convolution\n",
    "y = F.conv2d(x, w, stride=1, padding=1)\n",
    "loss = y.sum()\n",
    "\n",
    "# Backpropagate gradients using auto-differentiation\n",
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients of input and weights\n",
    "grad_x = x.grad\n",
    "grad_w = w.grad\n",
    "\n",
    "# Compute gradients of output tensor\n",
    "grad_y = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
    "\n",
    "# Compute gradients of input tensor and filters\n",
    "grad_x_manual = F.conv_transpose2d(grad_y,w, stride=1, padding=1)\n",
    "grad_w_manual = F.conv2d(x,grad_y,stride=1,padding=1)\n",
    "\n",
    "# Print the gradients computed using both methods to compare\n",
    "print(\"Gradient of y with respect to x using auto-differentiation:\")\n",
    "print(grad_x)\n",
    "print(\"Gradient of y with respect to x using manual computation:\")\n",
    "print(grad_x_manual)\n",
    "\n",
    "print(\"Gradient of y with respect to w using auto-differentiation:\")\n",
    "print(grad_w)\n",
    "print(\"Gradient of y with respect to w using manual computation:\")\n",
    "print(grad_w_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0801,  2.6186,  2.6186,  2.6186,  0.8259],\n",
      "          [-2.4839, -0.7851, -0.7851, -0.7851, -1.2002],\n",
      "          [-2.4839, -0.7851, -0.7851, -0.7851, -1.2002],\n",
      "          [-2.4839, -0.7851, -0.7851, -0.7851, -1.2002],\n",
      "          [-1.7288, -1.3754, -1.3754, -1.3754, -0.9006]]]])\n",
      "tensor([[[[-3.6176, -2.4300, -3.9069],\n",
      "          [-7.5963, -5.9993, -7.7322],\n",
      "          [-4.9380, -3.5094, -5.2451]]]])\n",
      "tensor([[[[-0.0801,  2.6186,  2.6186,  2.6186,  0.8259],\n",
      "          [-2.4839, -0.7851, -0.7851, -0.7851, -1.2002],\n",
      "          [-2.4839, -0.7851, -0.7851, -0.7851, -1.2002],\n",
      "          [-2.4839, -0.7851, -0.7851, -0.7851, -1.2002],\n",
      "          [-1.7288, -1.3754, -1.3754, -1.3754, -0.9006]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[[-3.6176, -2.4300, -3.9069],\n",
      "          [-7.5963, -5.9993, -7.7322],\n",
      "          [-4.9380, -3.5094, -5.2451]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define input tensor and filters\n",
    "x = torch.randn(1, 1, 5, 5, requires_grad=True)\n",
    "w = torch.randn(1, 1, 3, 3, requires_grad=True)\n",
    "\n",
    "# Perform convolution operation\n",
    "y = F.conv2d(x, w, stride=1, padding=1)\n",
    "\n",
    "# Define loss function\n",
    "loss = y.sum()\n",
    "# Backpropagate gradients using auto-differentiation\n",
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients of input and weights\n",
    "grad_x = x.grad\n",
    "grad_w = w.grad\n",
    "\n",
    "# Compute gradients of output tensor\n",
    "grad_y = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
    "\n",
    "# Compute gradients of input tensor and filters\n",
    "grad_x_manual = F.conv_transpose2d(grad_y,w, stride=1, padding=1)\n",
    "grad_w_manual = F.conv2d(x,grad_y,stride=1,padding=1)\n",
    "\n",
    "print(grad_x)\n",
    "print(grad_w)\n",
    "print(grad_x_manual)\n",
    "print(grad_w_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x using auto-differentiation:\n",
      "tensor([[[[0.9809, 0.2741, 0.2741, 0.2741, 0.6787],\n",
      "          [1.7208, 2.1069, 2.1069, 2.1069, 2.0114],\n",
      "          [1.7208, 2.1069, 2.1069, 2.1069, 2.0114],\n",
      "          [1.7208, 2.1069, 2.1069, 2.1069, 2.0114],\n",
      "          [1.6300, 2.4149, 2.4149, 2.4149, 1.2166]],\n",
      "\n",
      "         [[0.9261, 3.3335, 3.3335, 3.3335, 3.1076],\n",
      "          [1.4645, 3.2860, 3.2860, 3.2860, 2.1651],\n",
      "          [1.4645, 3.2860, 3.2860, 3.2860, 2.1651],\n",
      "          [1.4645, 3.2860, 3.2860, 3.2860, 2.1651],\n",
      "          [0.4526, 0.7800, 0.7800, 0.7800, 0.2822]],\n",
      "\n",
      "         [[1.3954, 0.5946, 0.5946, 0.5946, 0.3930],\n",
      "          [2.7697, 2.7162, 2.7162, 2.7162, 1.9890],\n",
      "          [2.7697, 2.7162, 2.7162, 2.7162, 1.9890],\n",
      "          [2.7697, 2.7162, 2.7162, 2.7162, 1.9890],\n",
      "          [2.6803, 3.7876, 3.7876, 3.7876, 2.6149]]]])\n",
      "Gradient of y with respect to x using manual computation:\n",
      "tensor([[[[0.9809, 0.2741, 0.2741, 0.2741, 0.6787],\n",
      "          [1.7208, 2.1069, 2.1069, 2.1069, 2.0114],\n",
      "          [1.7208, 2.1069, 2.1069, 2.1069, 2.0114],\n",
      "          [1.7208, 2.1069, 2.1069, 2.1069, 2.0114],\n",
      "          [1.6300, 2.4149, 2.4149, 2.4149, 1.2166]],\n",
      "\n",
      "         [[0.9261, 3.3335, 3.3335, 3.3335, 3.1076],\n",
      "          [1.4645, 3.2860, 3.2860, 3.2860, 2.1651],\n",
      "          [1.4645, 3.2860, 3.2860, 3.2860, 2.1651],\n",
      "          [1.4645, 3.2860, 3.2860, 3.2860, 2.1651],\n",
      "          [0.4526, 0.7800, 0.7800, 0.7800, 0.2822]],\n",
      "\n",
      "         [[1.3954, 0.5946, 0.5946, 0.5946, 0.3930],\n",
      "          [2.7697, 2.7162, 2.7162, 2.7162, 1.9890],\n",
      "          [2.7697, 2.7162, 2.7162, 2.7162, 1.9890],\n",
      "          [2.7697, 2.7162, 2.7162, 2.7162, 1.9890],\n",
      "          [2.6803, 3.7876, 3.7876, 3.7876, 2.6149]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "Gradient of y with respect to w using auto-differentiation:\n",
      "tensor([[[[-4.3024, -3.7767, -1.4200],\n",
      "          [-7.8859, -9.6537, -6.1335],\n",
      "          [-7.8937, -9.9299, -6.6063]],\n",
      "\n",
      "         [[-0.5746,  4.5232,  1.6823],\n",
      "          [-1.8471,  5.4913,  2.6900],\n",
      "          [-2.9981,  3.0621,  1.2294]],\n",
      "\n",
      "         [[ 0.8220,  0.3561, -0.6117],\n",
      "          [ 3.2618,  1.5745,  0.3377],\n",
      "          [ 1.5411,  1.4816,  1.8228]]]])\n",
      "Gradient of y with respect to w using manual computation:\n",
      "tensor([[[[-4.3024, -3.7767, -1.4200],\n",
      "          [-7.8859, -9.6537, -6.1335],\n",
      "          [-7.8937, -9.9299, -6.6063]]],\n",
      "\n",
      "\n",
      "        [[[-0.5746,  4.5232,  1.6823],\n",
      "          [-1.8471,  5.4913,  2.6900],\n",
      "          [-2.9981,  3.0621,  1.2294]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8220,  0.3561, -0.6117],\n",
      "          [ 3.2618,  1.5745,  0.3377],\n",
      "          [ 1.5411,  1.4816,  1.8228]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create some input data and a convolution kernel\n",
    "x = torch.randn(1, 3, 5, 5, requires_grad=True)\n",
    "w = torch.randn(1, 3, 3, 3, requires_grad=True)\n",
    "\n",
    "# Perform the convolution\n",
    "y = F.conv2d(x, w, stride=1, padding=1)\n",
    "loss = y.sum()\n",
    "\n",
    "# Backpropagate gradients using auto-differentiation\n",
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients of input and weights\n",
    "grad_x = x.grad\n",
    "grad_w = w.grad\n",
    "\n",
    "# Compute the gradients of y with respect to x and w using manual computation\n",
    "grad_y = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
    "grad_x_manual = F.conv_transpose2d(grad_y, w, stride=1, padding=1)\n",
    "grad_w_manual = F.conv2d( x.transpose(0,1),grad_y, stride=1, padding=1)\n",
    "\n",
    "# Print the gradients computed using both methods to compare\n",
    "print(\"Gradient of y with respect to x using auto-differentiation:\")\n",
    "print(grad_x)\n",
    "print(\"Gradient of y with respect to x using manual computation:\")\n",
    "print(grad_x_manual)\n",
    "\n",
    "print(\"Gradient of y with respect to w using auto-differentiation:\")\n",
    "print(grad_w)\n",
    "print(\"Gradient of y with respect to w using manual computation:\")\n",
    "print(grad_w_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x using auto-differentiation:\n",
      "tensor([[[[  3.4949,  -0.1608,  -0.1608,  -0.1608,   1.2496],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [ -0.6415,   0.8868,   0.8868,   0.8868,   3.6159]],\n",
      "\n",
      "         [[ -2.7936,  -7.6039,  -7.6039,  -7.6039,  -4.5853],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [  1.1513,  -5.0100,  -5.0100,  -5.0100,  -2.3244]],\n",
      "\n",
      "         [[ -0.2077,  -1.8835,  -1.8835,  -1.8835,  -2.2063],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  3.8845,   1.0679,   1.0679,   1.0679,  -0.2776]]],\n",
      "\n",
      "\n",
      "        [[[  3.4949,  -0.1608,  -0.1608,  -0.1608,   1.2496],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [ -0.6415,   0.8868,   0.8868,   0.8868,   3.6159]],\n",
      "\n",
      "         [[ -2.7936,  -7.6039,  -7.6039,  -7.6039,  -4.5853],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [  1.1513,  -5.0100,  -5.0100,  -5.0100,  -2.3244]],\n",
      "\n",
      "         [[ -0.2077,  -1.8835,  -1.8835,  -1.8835,  -2.2063],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  3.8845,   1.0679,   1.0679,   1.0679,  -0.2776]]],\n",
      "\n",
      "\n",
      "        [[[  3.4949,  -0.1608,  -0.1608,  -0.1608,   1.2496],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [ -0.6415,   0.8868,   0.8868,   0.8868,   3.6159]],\n",
      "\n",
      "         [[ -2.7936,  -7.6039,  -7.6039,  -7.6039,  -4.5853],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [  1.1513,  -5.0100,  -5.0100,  -5.0100,  -2.3244]],\n",
      "\n",
      "         [[ -0.2077,  -1.8835,  -1.8835,  -1.8835,  -2.2063],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  3.8845,   1.0679,   1.0679,   1.0679,  -0.2776]]]])\n",
      "Gradient of y with respect to x using manual computation:\n",
      "tensor([[[[  3.4949,  -0.1608,  -0.1608,  -0.1608,   1.2496],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [ -0.6415,   0.8868,   0.8868,   0.8868,   3.6159]],\n",
      "\n",
      "         [[ -2.7936,  -7.6039,  -7.6039,  -7.6039,  -4.5853],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [  1.1513,  -5.0100,  -5.0100,  -5.0100,  -2.3244]],\n",
      "\n",
      "         [[ -0.2077,  -1.8835,  -1.8835,  -1.8835,  -2.2063],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  3.8845,   1.0679,   1.0679,   1.0679,  -0.2776]]],\n",
      "\n",
      "\n",
      "        [[[  3.4949,  -0.1608,  -0.1608,  -0.1608,   1.2496],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [ -0.6415,   0.8868,   0.8868,   0.8868,   3.6159]],\n",
      "\n",
      "         [[ -2.7936,  -7.6039,  -7.6039,  -7.6039,  -4.5853],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [  1.1513,  -5.0100,  -5.0100,  -5.0100,  -2.3244]],\n",
      "\n",
      "         [[ -0.2077,  -1.8835,  -1.8835,  -1.8835,  -2.2063],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  3.8845,   1.0679,   1.0679,   1.0679,  -0.2776]]],\n",
      "\n",
      "\n",
      "        [[[  3.4949,  -0.1608,  -0.1608,  -0.1608,   1.2496],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [  1.3516,  -1.3661,  -1.3661,  -1.3661,   1.0601],\n",
      "          [ -0.6415,   0.8868,   0.8868,   0.8868,   3.6159]],\n",
      "\n",
      "         [[ -2.7936,  -7.6039,  -7.6039,  -7.6039,  -4.5853],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [ -2.1214, -10.2306, -10.2306, -10.2306,  -7.3572],\n",
      "          [  1.1513,  -5.0100,  -5.0100,  -5.0100,  -2.3244]],\n",
      "\n",
      "         [[ -0.2077,  -1.8835,  -1.8835,  -1.8835,  -2.2063],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  2.9592,   1.4115,   1.4115,   1.4115,  -0.1305],\n",
      "          [  3.8845,   1.0679,   1.0679,   1.0679,  -0.2776]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "Gradient of y with respect to w using auto-differentiation:\n",
      "tensor([[[[  4.9078,   1.3798,   1.9860],\n",
      "          [  6.1782,   0.9196,   1.3330],\n",
      "          [  6.5879,   1.6543,   2.7886]],\n",
      "\n",
      "         [[-17.4453, -19.1889, -10.6672],\n",
      "          [-15.0869, -17.8705,  -8.9647],\n",
      "          [ -6.8998,  -9.3039,  -3.6822]],\n",
      "\n",
      "         [[  3.8442,   3.8819,   3.4643],\n",
      "          [  3.3497,   7.5791,   4.5245],\n",
      "          [  3.3270,   7.1439,   7.0406]]],\n",
      "\n",
      "\n",
      "        [[[  4.9078,   1.3798,   1.9860],\n",
      "          [  6.1782,   0.9196,   1.3330],\n",
      "          [  6.5879,   1.6543,   2.7886]],\n",
      "\n",
      "         [[-17.4453, -19.1889, -10.6672],\n",
      "          [-15.0869, -17.8705,  -8.9647],\n",
      "          [ -6.8998,  -9.3039,  -3.6822]],\n",
      "\n",
      "         [[  3.8442,   3.8819,   3.4643],\n",
      "          [  3.3497,   7.5791,   4.5245],\n",
      "          [  3.3270,   7.1439,   7.0406]]],\n",
      "\n",
      "\n",
      "        [[[  4.9078,   1.3798,   1.9860],\n",
      "          [  6.1782,   0.9196,   1.3330],\n",
      "          [  6.5879,   1.6543,   2.7886]],\n",
      "\n",
      "         [[-17.4453, -19.1889, -10.6672],\n",
      "          [-15.0869, -17.8705,  -8.9647],\n",
      "          [ -6.8998,  -9.3039,  -3.6822]],\n",
      "\n",
      "         [[  3.8442,   3.8819,   3.4643],\n",
      "          [  3.3497,   7.5791,   4.5245],\n",
      "          [  3.3270,   7.1439,   7.0406]]]])\n",
      "Gradient of y with respect to w using manual computation:\n",
      "tensor([[[[  4.9078,   1.3798,   1.9860],\n",
      "          [  6.1782,   0.9196,   1.3330],\n",
      "          [  6.5879,   1.6543,   2.7886]],\n",
      "\n",
      "         [[  4.9078,   1.3798,   1.9860],\n",
      "          [  6.1782,   0.9196,   1.3330],\n",
      "          [  6.5879,   1.6543,   2.7886]],\n",
      "\n",
      "         [[  4.9078,   1.3798,   1.9860],\n",
      "          [  6.1782,   0.9196,   1.3330],\n",
      "          [  6.5879,   1.6543,   2.7886]]],\n",
      "\n",
      "\n",
      "        [[[-17.4453, -19.1889, -10.6672],\n",
      "          [-15.0869, -17.8705,  -8.9647],\n",
      "          [ -6.8998,  -9.3039,  -3.6822]],\n",
      "\n",
      "         [[-17.4453, -19.1889, -10.6672],\n",
      "          [-15.0869, -17.8705,  -8.9647],\n",
      "          [ -6.8998,  -9.3039,  -3.6822]],\n",
      "\n",
      "         [[-17.4453, -19.1889, -10.6672],\n",
      "          [-15.0869, -17.8705,  -8.9647],\n",
      "          [ -6.8998,  -9.3039,  -3.6822]]],\n",
      "\n",
      "\n",
      "        [[[  3.8442,   3.8819,   3.4643],\n",
      "          [  3.3497,   7.5791,   4.5245],\n",
      "          [  3.3270,   7.1439,   7.0406]],\n",
      "\n",
      "         [[  3.8442,   3.8819,   3.4643],\n",
      "          [  3.3497,   7.5791,   4.5245],\n",
      "          [  3.3270,   7.1439,   7.0406]],\n",
      "\n",
      "         [[  3.8442,   3.8819,   3.4643],\n",
      "          [  3.3497,   7.5791,   4.5245],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          [  3.3270,   7.1439,   7.0406]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create some input data and a convolution kernel\n",
    "x = torch.randn(3, 3, 5, 5, requires_grad=True)\n",
    "w = torch.randn(3, 3, 3, 3, requires_grad=True)\n",
    "\n",
    "# Perform the convolution\n",
    "y = F.conv2d(x, w, stride=1, padding=1)\n",
    "loss = y.sum()\n",
    "\n",
    "# Backpropagate gradients using auto-differentiation\n",
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients of input and weights\n",
    "grad_x = x.grad\n",
    "grad_w = w.grad\n",
    "\n",
    "# Compute the gradients of y with respect to x and w using manual computation\n",
    "grad_y = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
    "grad_x_manual = F.conv_transpose2d(grad_y, w, stride=1, padding=1)\n",
    "grad_w_manual = F.conv2d(x.transpose(0, 1), grad_y, stride=1, padding=1)\n",
    "\n",
    "# Print the gradients computed using both methods to compare\n",
    "print(\"Gradient of y with respect to x using auto-differentiation:\")\n",
    "print(grad_x)\n",
    "print(\"Gradient of y with respect to x using manual computation:\")\n",
    "print(grad_x_manual)\n",
    "\n",
    "print(\"Gradient of y with respect to w using auto-differentiation:\")\n",
    "print(grad_w)\n",
    "print(\"Gradient of y with respect to w using manual computation:\")\n",
    "print(grad_w_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.7638, -0.0145, -0.0145, -0.0145,  1.3298],\n",
       "          [ 3.2370,  0.2800,  0.2800,  0.2800,  1.2824],\n",
       "          [ 3.2370,  0.2800,  0.2800,  0.2800,  1.2824],\n",
       "          [ 3.2370,  0.2800,  0.2800,  0.2800,  1.2824],\n",
       "          [ 4.1476,  0.9790,  0.9790,  0.9790,  1.5422]],\n",
       "\n",
       "         [[-2.4175, -3.8301, -3.8301, -3.8301, -1.9125],\n",
       "          [-1.0575, -3.4531, -3.4531, -3.4531, -2.4171],\n",
       "          [-1.0575, -3.4531, -3.4531, -3.4531, -2.4171],\n",
       "          [-1.0575, -3.4531, -3.4531, -3.4531, -2.4171],\n",
       "          [ 1.0559,  1.5893,  1.5893,  1.5893,  2.1449]],\n",
       "\n",
       "         [[ 0.1473, -5.8035, -5.8035, -5.8035, -4.9593],\n",
       "          [ 0.0100, -7.0121, -7.0121, -7.0121, -5.2930],\n",
       "          [ 0.0100, -7.0121, -7.0121, -7.0121, -5.2930],\n",
       "          [ 0.0100, -7.0121, -7.0121, -7.0121, -5.2930],\n",
       "          [-0.8092, -5.6235, -5.6235, -5.6235, -2.6732]]],\n",
       "\n",
       "\n",
       "        [[[ 2.7638, -0.0145, -0.0145, -0.0145,  1.3298],\n",
       "          [ 3.2370,  0.2800,  0.2800,  0.2800,  1.2824],\n",
       "          [ 3.2370,  0.2800,  0.2800,  0.2800,  1.2824],\n",
       "          [ 3.2370,  0.2800,  0.2800,  0.2800,  1.2824],\n",
       "          [ 4.1476,  0.9790,  0.9790,  0.9790,  1.5422]],\n",
       "\n",
       "         [[-2.4175, -3.8301, -3.8301, -3.8301, -1.9125],\n",
       "          [-1.0575, -3.4531, -3.4531, -3.4531, -2.4171],\n",
       "          [-1.0575, -3.4531, -3.4531, -3.4531, -2.4171],\n",
       "          [-1.0575, -3.4531, -3.4531, -3.4531, -2.4171],\n",
       "          [ 1.0559,  1.5893,  1.5893,  1.5893,  2.1449]],\n",
       "\n",
       "         [[ 0.1473, -5.8035, -5.8035, -5.8035, -4.9593],\n",
       "          [ 0.0100, -7.0121, -7.0121, -7.0121, -5.2930],\n",
       "          [ 0.0100, -7.0121, -7.0121, -7.0121, -5.2930],\n",
       "          [ 0.0100, -7.0121, -7.0121, -7.0121, -5.2930],\n",
       "          [-0.8092, -5.6235, -5.6235, -5.6235, -2.6732]]],\n",
       "\n",
       "\n",
       "        [[[ 2.7638, -0.0145, -0.0145, -0.0145,  1.3298],\n",
       "          [ 3.2370,  0.2800,  0.2800,  0.2800,  1.2824],\n",
       "          [ 3.2370,  0.2800,  0.2800,  0.2800,  1.2824],\n",
       "          [ 3.2370,  0.2800,  0.2800,  0.2800,  1.2824],\n",
       "          [ 4.1476,  0.9790,  0.9790,  0.9790,  1.5422]],\n",
       "\n",
       "         [[-2.4175, -3.8301, -3.8301, -3.8301, -1.9125],\n",
       "          [-1.0575, -3.4531, -3.4531, -3.4531, -2.4171],\n",
       "          [-1.0575, -3.4531, -3.4531, -3.4531, -2.4171],\n",
       "          [-1.0575, -3.4531, -3.4531, -3.4531, -2.4171],\n",
       "          [ 1.0559,  1.5893,  1.5893,  1.5893,  2.1449]],\n",
       "\n",
       "         [[ 0.1473, -5.8035, -5.8035, -5.8035, -4.9593],\n",
       "          [ 0.0100, -7.0121, -7.0121, -7.0121, -5.2930],\n",
       "          [ 0.0100, -7.0121, -7.0121, -7.0121, -5.2930],\n",
       "          [ 0.0100, -7.0121, -7.0121, -7.0121, -5.2930],\n",
       "          [-0.8092, -5.6235, -5.6235, -5.6235, -2.6732]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x_manual = F.conv2d(grad_y,w.flip([2,3]),stride=1, padding=1)\n",
    "grad_x_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

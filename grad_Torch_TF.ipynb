{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative with respect to x: \n",
      "tensor([[ 1.,  0., -1.],\n",
      "        [ 2.,  0., -2.],\n",
      "        [ 1.,  0., -1.]])\n",
      "Derivative with respect to f: \n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define input data and filters\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], requires_grad=True)\n",
    "f = torch.tensor([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], requires_grad=True)\n",
    "\n",
    "# Define the forward pass function\n",
    "def conv2d(x, f):\n",
    "    return torch.nn.functional.conv2d(x.view(1, 1, 3, 3), f.view(1, 1, 3, 3), padding=0)\n",
    "\n",
    "# Define the loss function (sum of the output values)\n",
    "def loss(y):\n",
    "    return y.sum()\n",
    "\n",
    "# Compute the derivative of the loss with respect to x and f using autograd\n",
    "y = conv2d(x, f)\n",
    "L = loss(y)\n",
    "grads = torch.autograd.grad(L, [x, f])\n",
    "\n",
    "# Print the derivative values\n",
    "print(\"Derivative with respect to x: \")\n",
    "print(grads[0])\n",
    "print(\"Derivative with respect to f: \")\n",
    "print(grads[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative with respect to x: \n",
      "tensor([[[[ 1.,  0., -1.],\n",
      "          [ 2.,  0., -2.],\n",
      "          [ 1.,  0., -1.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Derivative with respect to f: \n",
      "tensor([[[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define input data and filters\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], requires_grad=True)\n",
    "f = torch.tensor([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], requires_grad=True)\n",
    "\n",
    "# Define the forward pass function\n",
    "def conv2d(x, f):\n",
    "    return torch.nn.functional.conv2d(x.view(1, 1, 3, 3), f.view(1, 1, 3, 3), padding=0)\n",
    "\n",
    "# Define the loss function (sum of the output values)\n",
    "def loss(y):\n",
    "    return y.sum()\n",
    "\n",
    "# Compute the derivative of the loss with respect to x and f using PyTorch's low-level functions\n",
    "y = conv2d(x, f)\n",
    "L = loss(y)\n",
    "\n",
    "# Compute the derivative of L with respect to y\n",
    "grad_y = torch.ones_like(y)\n",
    "#grad_y *= L\n",
    "\n",
    "# Compute the derivative of L with respect to x and f using conv2d_transpose\n",
    "grad_x = torch.nn.functional.conv_transpose2d(grad_y, f.view(1, 1, 3, 3), padding=0)\n",
    "grad_f = torch.nn.functional.conv2d(x.view(1, 1, 3, 3), grad_y, padding=0)\n",
    "\n",
    "# Print the derivative values\n",
    "print(\"Derivative with respect to x: \")\n",
    "print(grad_x)\n",
    "print(\"Derivative with respect to f: \")\n",
    "print(grad_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[[-8.]]]], shape=(1, 1, 1, 1), dtype=float32)\n",
      "Derivative with respect to x: \n",
      "tf.Tensor(\n",
      "[[ 1.  0. -1.]\n",
      " [ 2.  0. -2.]\n",
      " [ 1.  0. -1.]], shape=(3, 3), dtype=float32)\n",
      "Derivative with respect to f: \n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define input data and filters\n",
    "x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], dtype=tf.float32)\n",
    "f = tf.constant([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], dtype=tf.float32)\n",
    "\n",
    "# Define the forward pass function\n",
    "def conv2d(x, f):\n",
    "    return tf.nn.conv2d(tf.reshape(x, [1, 3, 3, 1]), tf.reshape(f, [3, 3, 1, 1]), strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "# Define the loss function (sum of the output values)\n",
    "def loss(y):\n",
    "    return tf.reduce_sum(y)\n",
    "\n",
    "# Compute the derivative of the loss with respect to x and f using tape gradient\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x, f])\n",
    "    y = conv2d(x, f)\n",
    "    #L = loss(y)\n",
    "    print(y)\n",
    "grads = tape.gradient(y, [x, f])\n",
    "\n",
    "# Print the derivative values\n",
    "print(\"Derivative with respect to x: \")\n",
    "print(grads[0])\n",
    "print(\"Derivative with respect to f: \")\n",
    "print(grads[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of convolution: \n",
      "tf.Tensor([[[[-8.]]]], shape=(1, 1, 1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define input data and filters\n",
    "x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], dtype=tf.float32)\n",
    "f = tf.constant([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], dtype=tf.float32)\n",
    "\n",
    "# Perform the 2D convolution\n",
    "y = tf.nn.conv2d(tf.reshape(x, [1, 3, 3, 1]), tf.reshape(f, [3, 3, 1, 1]), strides=[1, 1, 1, 1], padding='VALID')\n",
    "print(\"Output of convolution: \")\n",
    "print(y)\n",
    "\n",
    "# Define the derivative of the convolution operation\n",
    "x_grad = tf.nn.conv2d_transpose(y, tf.reshape(f, [3, 3, 1, 1]), tf.shape(x), strides=[1, 1, 1, 1], padding='VALID')\n",
    "f_grad = tf.nn.conv2d(tf.reshape(x, [1, 3, 3, 1]), tf.transpose(y, perm=[1, 2, 0, 3]), strides=[1, 1, 1, 1], padding='VALID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\n",
       "array([[[[ -8.],\n",
       "         [  0.],\n",
       "         [  8.]],\n",
       "\n",
       "        [[-16.],\n",
       "         [  0.],\n",
       "         [ 16.]],\n",
       "\n",
       "        [[ -8.],\n",
       "         [  0.],\n",
       "         [  8.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def convolve2d_gradient(x, w, d):\n",
    "    \"\"\"\n",
    "    Computes the gradient of 2D convolution with respect to x (dx) and the filter (dw).\n",
    "\n",
    "    Args:\n",
    "    x: 2D input array of shape (H, W)\n",
    "    w: 2D filter array of shape (FH, FW)\n",
    "    d: 2D output gradient array of shape (OH, OW)\n",
    "\n",
    "    Returns:\n",
    "    dx: 2D gradient of x array of shape (H, W)\n",
    "    dw: 2D gradient of w array of shape (FH, FW)\n",
    "    \"\"\"\n",
    "\n",
    "    # Flip the filter in both directions for cross-correlation\n",
    "    w_flipped = np.flip(np.flip(w, axis=0), axis=1)\n",
    "\n",
    "    # Compute the gradient of x using the transposed filter\n",
    "    dx = convolve2d(d, w_flipped, mode='full')[:x.shape[0], :x.shape[1]]\n",
    "\n",
    "    # Compute the gradient of w using the input and output gradients\n",
    "    dw = convolve2d(np.flip(np.flip(x, axis=0), axis=1), d, mode='valid')\n",
    "\n",
    "    return dx, dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x:\n",
      "[[1 2 0]\n",
      " [3 5 2]\n",
      " [0 3 4]]\n",
      "Gradient of w:\n",
      "[[77 67]\n",
      " [47 37]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "w = np.array([[1, 0], [0, 1]])\n",
    "d = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "dx, dw = convolve2d_gradient(x, w, d)\n",
    "\n",
    "print(\"Gradient of x:\")\n",
    "print(dx)\n",
    "\n",
    "print(\"Gradient of w:\")\n",
    "print(dw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rot180(w):\n",
    "    \"\"\"\n",
    "    Roate by 180 degrees\n",
    "    \"\"\"\n",
    "    return torch.flip(w, dims=[2, 3])\n",
    "\n",
    "def pad_to_full_conv2d(x, w_size):\n",
    "    \"\"\"\n",
    "    Pad x, such that using a 'VALID' convolution in PyTorch is the same\n",
    "    as using a 'FULL' convolution.\n",
    "    \"\"\"\n",
    "    padding = w_size - 1\n",
    "    return F.pad(x, (padding, padding, padding, padding), mode='constant', value=0)\n",
    "\n",
    "def NHWC_to_HWIO(out):\n",
    "    \"\"\"\n",
    "    Converts [batch, in_channels, in_height, in_width]\n",
    "    to       [out_channels, in_channels, filter_height, filter_width]\n",
    "    \"\"\"\n",
    "    return out.permute(3, 2, 1, 0)\n",
    "\n",
    "# sizes, fixed strides, in_channel, out_channel be 1 for now\n",
    "x_size = 4\n",
    "w_size = 3  # use an odd number here\n",
    "x_shape = (1, 1, x_size, x_size)\n",
    "w_shape = (1, 1, w_size, w_size)\n",
    "out_shape = (1, 1, x_size - w_size + 1, x_size - w_size + 1)\n",
    "strides = (1, 1)\n",
    "\n",
    "# numpy value\n",
    "x_np = torch.randint(10, size=x_shape, dtype=torch.float32)\n",
    "w_np = torch.randint(10, size=w_shape, dtype=torch.float32)\n",
    "out_scale_np = torch.randint(10, size=out_shape, dtype=torch.float32)\n",
    "\n",
    "# pytorch forward\n",
    "x = torch.tensor(x_np, requires_grad=True)\n",
    "w = torch.tensor(w_np, requires_grad=True)\n",
    "out = F.conv2d(x, w, stride=strides, padding=0)\n",
    "out_scale = torch.tensor(out_scale_np, requires_grad=True)\n",
    "f = torch.sum(out * out_scale)\n",
    "\n",
    "# pytorch backward\n",
    "f.backward(retain_graph=True)\n",
    "d_out = x.grad.detach()\n",
    "\n",
    "# 4 different ways to compute d_x\n",
    "d_x = x.grad\n",
    "d_x_manual = F.conv2d(d_out, w.flip([2, 3]), stride=strides, padding=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1788., 1604.],\n",
       "          [2218., 1998.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_x_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 27.,  78.,  31.,  40.],\n",
       "          [ 63.,  81., 105.,  71.],\n",
       "          [ 18.,  54.,  93.,  45.],\n",
       "          [  0.,  18.,  27.,   9.]]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define input tensor and filters\n",
    "x = Variable(torch.randn(3, 1, 3, 3), requires_grad=True)\n",
    "w = Variable(torch.randn(3, 1, 3, 3), requires_grad=True)\n",
    "\n",
    "# Perform convolution operation\n",
    "y = F.conv2d(x, w, stride=1, padding=1)\n",
    "\n",
    "# Define loss function\n",
    "loss = y.sum()\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients of input and weights\n",
    "grad_x = x.grad\n",
    "grad_w = w.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -8.9880, -10.5122,  -4.5570],\n",
       "          [ -5.1470,  -7.5202,  -4.7913],\n",
       "          [  2.8497,   0.1662,  -2.4810]]],\n",
       "\n",
       "\n",
       "        [[[ -8.9880, -10.5122,  -4.5570],\n",
       "          [ -5.1470,  -7.5202,  -4.7913],\n",
       "          [  2.8497,   0.1662,  -2.4810]]],\n",
       "\n",
       "\n",
       "        [[[ -8.9880, -10.5122,  -4.5570],\n",
       "          [ -5.1470,  -7.5202,  -4.7913],\n",
       "          [  2.8497,   0.1662,  -2.4810]]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -5.2912,  -9.7557,  -5.3568],\n",
       "          [ -6.1425, -11.4750,  -5.6802],\n",
       "          [ -3.0966,  -5.6679,  -2.7107]]],\n",
       "\n",
       "\n",
       "        [[[ -5.2912,  -9.7557,  -5.3568],\n",
       "          [ -6.1425, -11.4750,  -5.6802],\n",
       "          [ -3.0966,  -5.6679,  -2.7107]]],\n",
       "\n",
       "\n",
       "        [[[ -5.2912,  -9.7557,  -5.3568],\n",
       "          [ -6.1425, -11.4750,  -5.6802],\n",
       "          [ -3.0966,  -5.6679,  -2.7107]]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define input tensor and filters\n",
    "x = torch.randn(1, 1, 4, 4, requires_grad=True)\n",
    "w = torch.randn(1, 1, 3, 3, requires_grad=True)\n",
    "\n",
    "# Perform convolution operation\n",
    "y = F.conv2d(x, w, stride=1, padding=1)\n",
    "\n",
    "# Define loss function\n",
    "loss = y.sum()\n",
    "\n",
    "# Compute gradients of output tensor\n",
    "grad_y = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
    "\n",
    "# Compute gradients of input tensor and filters\n",
    "grad_x = F.conv_transpose2d(grad_y,w, stride=1, padding=1)\n",
    "grad_w = F.conv2d(x,grad_y,stride=1,padding=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4, 4])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.5836, -4.1881, -4.1881, -3.6721],\n",
       "          [-1.2203, -2.8034, -2.8034, -2.5446],\n",
       "          [-1.2203, -2.8034, -2.8034, -2.5446],\n",
       "          [-0.6830, -1.5558, -1.5558, -1.9757]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.7657, -4.4332, -4.4177],\n",
       "          [-3.8632, -6.1062, -5.6357],\n",
       "          [-3.9322, -4.1167, -3.1665]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients of input and weights\n",
    "grad_x = x.grad\n",
    "grad_w = w.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.5836, -4.1881, -4.1881, -3.6721],\n",
       "          [-1.2203, -2.8034, -2.8034, -2.5446],\n",
       "          [-1.2203, -2.8034, -2.8034, -2.5446],\n",
       "          [-0.6830, -1.5558, -1.5558, -1.9757]]]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.7657, -4.4332, -4.4177],\n",
       "          [-3.8632, -6.1062, -5.6357],\n",
       "          [-3.9322, -4.1167, -3.1665]]]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_x = F.conv2d(grad_y,w.flip([2,3]),padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.5836, -4.1881, -4.1881, -3.6721],\n",
       "          [-1.2203, -2.8034, -2.8034, -2.5446],\n",
       "          [-1.2203, -2.8034, -2.8034, -2.5446],\n",
       "          [-0.6830, -1.5558, -1.5558, -1.9757]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
